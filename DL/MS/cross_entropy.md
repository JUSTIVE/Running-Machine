# 크로스 엔트로피 조지기

학습된(또는 학습 중인) 뉴럴 네트워크에 입력값을 넣은 후 softmax를 하게 되면 전체 출력 벡터의 원소의 합이 1인 벡터(`logits`)를 구할 수 있다.
> $$f_j(z) = \frac{x^{z_j}}{\sum_k{x^{z_k}}}$$

이에, 원래 트레이닝 셋의 벡터와의 차이를 구하기 위해 Cost function을 이용하는데, 이는 다음과 같다.
> $$H(p,q) = -\sum_{x}{p(x)log q(x)}$$