# 역전파 알고리즘

우리는 어떻게 뉴럴 네트워크가 입력값을 고정된 가중치에 의해 결정적인 출력값에 사상하는지를 보았다. 뉴럴 네트워크의 구조(피드-포워드, 히든레이어의 수, 레이어당 뉴런의 수)가 정의되고, 각 뉴런의 활성화 함수가 선택된 이상, 우리는 이러한 네트워크 상의 각각의 뉴런의 상태를 결정할 가중치들을 설정할 필요가 있다. 우리는 1 레이어- 네트워크에서 어떻게 동작하는 지 보고, 이를 깊은 전방향 네트워크에 어떻게 확장할 지를 확인할 것이다. 딥 뉴럴 네트워크에서 가중치를 설정하는 알고리즘의 이름은 역전파 알고리즘이라 불리며, 우리는 이것에 대해 논하고, 이 알고리즘에 대해 설명할 것이다. 우선, 단일 레이어 - 네트워크에서 확인해보자.

우리가 이해해야할 일반적인 개념은 다음과 같다: 모든 뉴럴 네트워크는 어떤 함수에의 근사이며, 이에 각 뉴럴네트워크는 요구하는 함수와 같지 않은 대신, 몇몇 값들이 다를 것이다. 이러한 값들은 오차 **error** 라 불리며, 우리의 목표는 이 오차값을 줄이는 것이다. 오차가 뉴럴 네트워크의 가중치의 함수이기 때문에, 우리는 가중치를 이용하여 오차를 줄일 것이다. 오차 함수는 많은 가중치들의 함수이다. 이는 많은 변수를 포함한 함수이다. 수학적으로, 이 함수가 0을 가지는 값의 집합은 초평면을 정의하며, 이 평면상의 최소점들을 찾고 곡선을 따라 최소지점으로 가야 합니다.

## 선형 회귀

우리는 이미 첫 장에서 선형 회귀에 대해 소개했으나, 우리는 이제 많은 변수를 다루고 있기 때문에, 행렬 표기를 통해 간단하게 나타낼 것이다. `x`를 입력이라 하자. 우리는 `x`를 하나의 벡터라 생각할 수 있다. 선형 회귀의 경우, 우리는 단일 출력 뉴런 `y`를 고려할 것이다. 가중치의 집합 `w`는 `x` 와 같은 크기의 벡터 차원을 가지는 벡터일 것이다. 활성 값은 내부 산물인 `<x,w>`로 정의된다.

우리가 각각의 입력 `x`에 대해 우리는 목표값 `t`를 가지길 원한다. 각각의 입력 `x`에 대해서 뉴럴 네트워크는 정의된 활성 함수에 의해 `y`의 출력값을 가질 것이나, 이 경우 (y-t)의 절댓값은 입력값 `x`에 대해서 우리의 예측 값과 실제 값의 차이를 나타낸다. 만약 m개의 입력값 ${x_i}$들에 대해, 각가의 입력값은 목표값 $t_i$를 가질 것이다. 이 경우에, 우리는 오차를 평균제곱오차 $\sum_i(y^i-t^i)^2$를 통해 구할 것이다. 이때의 $y^2$는 `w`의 함수 값이다. 이 `w`에 대한 함수의 오류는 보통 J(w)로 나타내어진다. 우리가 앞에서 언급했듯이, 이는 `w`와 같은 차원의 초공간을 나타내고, 각각의 $w_j$에 대해 우리는 평면상의 최저점을 향하는 곡선을 찾아야 한다. 이 곡선이 방향은 다음과 같이구할 수 있다.
> ![direction of the error on hyper-surface](https://latex.codecogs.com/svg.latex?\overrightarrow{d}&space;=&space;\frac{\delta&space;\sum_i(y^i-t^i)^2}{\delta&space;w_j})