# Word To Vector

## 언어 모델링

언어 모델링의 목표는 대개 단어 확률을 계산하는 것이다.
특히 음성 인식, 광학 문자 인식, 기계 번역 및 맞춤법 교정같은 프로그램에서 매우 중요하다.

따라서 좋은 언어 모델은 대화 어떤 구문이 가장 올바른지 구별할 수 있다.

## Word 기반 모델

### `N-grams`

N-gram은 문자열에서 `N개의 연속된 요소를 추출`하는 방법이다.
각 시퀸스의 확률에 대한 추론(w1, ... wm)은 일반적으로 실행 못한다.
P(w1, ..., wm)의 결합 확률을 계산하려면 다음과 같은 `체인 규칙`을 적용하면된다.

$$P(w_1 ... , 2_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)* ... *P(w_m|w_1, ... , w_{m-1})$$

특정 초기 단어가 부여된 이후 단어의 확률은 데이터로부터 추정하기가 어렵다. n-gram은 순차적 단어가 같을 확률만 모델링한다.
n-gram은 길이가 n인 다른 문자를 나타내기 위해 사용될 수 있다.

이 공동 분포에 대한 추론은 여러 개별 부품에서 합동 분포를 분할하는 N-gram 모델을 통해 근사적으로 추정할 수 있다.

예를 들어 "The quick brown fox"라는 단어에서

- 1-gram: "The," "quick," "brown," and "fox" (also known as unigram)
- 2-grams: "The quick," "quick brown," and "brown fox" (also known as bigram)
- 3-grams: "The quick brown" and "quick brown fox" (also known as trigram)
- 4-grams: "The quick brown fox"

방대한 양의 텍스트가 있는 경우, 특정 n개(일반적으로 2~4)까지  
모든 n개 그래프를 찾아 해당 문자열에서 각 n 그램의 발생을 카운트할 수 있다.  
이전 n-1 단어에서 각 n-그램의 마지막 단어의 확률을 추정할 수 있다.

- ![n-gram](word_to_vector_pic\n-gram.png)

i번째 단어가 이전 n-1 단어에만 의존한다는 독립성 가정은 공동 분포의 근사치에 사용할 수 있다.
예르 들어, 유니그램의 경우 다음과 같은 방법으로 결합 분포를 추정할 수 있다.

$$P(w_1 ... , w_m) = P(w_1)*P(w_2)*P(w_3)* ... *P(w_m)$$

tigram의 경우

$$P(w_1 ... , 2_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_2,w_1)* ... *P(w_m|w_{m-2},w_{m-1})$$

따라서, 어휘 크기에 기반해 n-gram의 수가 n과 함께 기하 급수적으로 증가 함을 알 수 있다.  (이를 이를 `차원의 저주`이라고 한다.)

예를 들어 100 단어가 포함된 경우 5-gram의 수는 $100^5 = 10,000,000,000$이다.  
따라서 모든 확률을 저장하기 위해서는 매우 큰 저장 장치가 필요하며 n의 큰 값에 대한 n-그램 확률 추정치를 만들기 위하여 매우 큰 텍스트 덩어리가 필요하다.

조합당 최소 하나의 예(n-graph modeling의 경우)가 필요할 때 발생한다.
n이 클수록 원래 분포에 대한 근사치 및 n-gram 확률에 대한 더 많은 데이터가 필요하다.

## Nerual language models

n-gram의 차원의 저주를 극복하는 방법은 단어의 더 낮은 차원의 분산 표현을 학습하는 것이다.

![낮은 차원 분산](word_to_vector_pic\under_dimension.png)
> 어휘에서 V-단어는 V크기의 핫 인코딩 벡터로 변환된다.(각 단어는 유니크하게 인코딩된다.)  
그런 다음 임베딩 함수는 V 차원 공간을 크기 D(위에서는 D=4)의 분산 표현으로 변환합니다.

이 방법 핵심은 `학습된 임베딩 함수가 단어에 대한 의미 정보를 학습`하는 것이다.  
이는 어휘의 각 단어를 연속적인 가치를 지닌 벡터 표현인 단어 임베딩과 연결시킨다.

각 단어는 단어의 문법적 또는 의미론적 속성에 다른 차원이 해당되는 이 단어의 포함 영역에 해당된다.  
`목표는 이 임베딩 영역에서 서로 가깝게 단어가 유사한 의미를 갖도록 하는 것`이다.  이 방법으로 일부 단어가 의미론적으로 유사한 정보가 언어 모델에 의해 악용될 수 있다.  

예를 들어 "여우", "고양이"가 의미 상으로 관련되어 있고 "재빠른 갈색 여우"와 "재빠른 갈색 고양이"가 유효한 구문이라는 것을 알 수 있다.
이후 일련의 단어를  단어의 특성을 포착하는 일련으이 벡터로 변환할 수 있다.

싱경망을 통해 언어 모델을 모델링하고 묵시적으로 내장 함수를 학습하는 것이 가능하다. 만약 n-1 단어들의 순서를 주어진 신경 네트워크를 배우는 것이다. 이때 $w_{t-n+1}, ..., w_{t-1}$ 는 다음 단어의 확률 분포, 즉 wt를 출력하고 한다.

임베딩 계층은 단어 $w_i$를 한 번 열렬하게 표현한 다음 임베딩 행렬 C와 곱하여 그 임베딩으로 변환한다. 이 계산은 테이블 조회로 효율적으로 구현할 수 있다.

`임베딩 행렬 C는 모든 단어에 걸쳐 공유`되므로 모든 단어는 동일한 임베딩 함수를 사용한다. C는 `V*D 행렬`로 표현된다. 여기서 V는 어휘의 크기, D는 임베딩 크기이다. `임베딩의 결과는 히든 레이어로 연결`된다. 그 후 바이어스 b와 tanh와 같은 비선형 함수가 적용될 수 있다. 히든 레이어의 출력은 $z=tanh(concat(w_{t-n+1}, ..., w_{t-1}))$ 함수로 표현된다.

히든 레이어에서 `히든 레이어와 U`를 곱하여 다음 단어 $w_t$의 확률 분포를 출력할 수 있다. 이렇게 하면 히든 레이어에 단어 공간에 매핑하고 바이어스 b를 추가하고 softmax 함수를 적용하여 확률 분포를 얻는다. 최종 계층은 `softmax(z*U + b)`를 계산한다.

이 네트워크는 아래 그림과 같다.

![임베딩 네트워크](word_to_vector_pic\embedding_net.png)

이 모델은 어휘의 `모든 단어의 임베딩과 단어의 연속에 대한 확률 함수의 모델을 동시에 습득`한다.  
이러한 분산 표현 덕분에 훈련 중 보이지 않는 일련의 단어로 이 확률 함수를 일반화할 수 있다.  
테스트 세트의 특정 단어 조합은 트레이닝 세트에는 보이지 않을 수 있으나 유사한 내장 기능을 가진 시퀸스는 트레이닝 중에 더 많이 표시될 수 있다.

아래는 일부 단어 포함에 대한 2D 투영을 보여준다.  

![2D 투영](word_to_vector_pic\2D_word_relation.png)  

여기에서 의미적으로 가까운 단어가 임베딩 공간에서 서로 가까운 것을 볼 수 있다.

단어 임베딩은 대규모 텍스트 데이터에 대해 비감독식 훈련이 될 수 있다.
이 방법으로 단어 사이에 일반적인 의미 정보를 포착할 수 있다.
그 결과로 `임베딩은 아직 분류가 안된 데이터들의 다른 작업의 성능을 향상` 시킬 수 있다.  
예를 들어 뉴스 기사에 내용을 분류하는 분류자는 one-hot 인코딩 벡터 대신에 이전에 습득한 단어 삽입을 사용하여 훈련될 수 있다.  

이러한 장점 덕분에 많은 연구가 단어의 연속에 대한 확률 함수를 학습하는 것에 초점을 두지 않고 더 나은 단어 임베딩을 만드는 것이 탄생하였다.

놀랍게도 단어 임베딩은 단어들 간 유추를 차이로 포착 할 수 있다.
예를 들어 "여성"과 "남성"의 임베딩 차이가 성을 인코딩하고 이 차이는 "여왕", "왕"과 같은 다른 성 관련 단어에서 동일하게 분류할 수 있다.

![단어 임베딩](word_to_vector_pic\word_embedding.png)