# Language model

음성 인식, 광학 문자 인식, 기계 번역 및 맞춤법 교정과 같은 많은 다른 응용 프로그램에 중요합니다. 예를 들어, 미국 영어의 경우 두 문구가 멋진 해변을 난파하고 발음을 인식하는 발음은 거의 동일하지만 각각의 의미는 완전히 다릅니다. 좋은 언어 모델은 대화의 컨텍스트를 기반으로 어느 구문이 가장 올바른지를 구별 할 수 있습니다. 이 섹션에서는 단어 및 문자 수준 언어 모델의 개요와 RNN을 사용하여 언어 모델을 작성하는 방법에 대해 설명합니다.

## word base model

단어 기반 언어 모델은 일련의 단어에 대한 확률 분포를 정의합니다. 길이가 m 인 단어들의 시퀀스가 주어지면, 단어의 전체 시퀀스에 확률 P (w1, ..., wm)을 할당합니다. 이 확률의 적용은 두 가지입니다. 자연 언어 처리 응용 프로그램에서 다른 구문의 가능성을 추정하는 데 사용할 수 있습니다. 또는 우리는 그들을 생성하여 새로운 텍스트를 생성 할 수 있습니다.

## n-gram

긴 시퀀스의 확률에 대한 추론, 예를 들어 w1, ..., wm은 일반적으로 실행 불가능하다. P (w1, ..., wm)의 결합 확률을 계산하려면 다음 체인 규칙을 적용하면됩니다.

$P(w_1...w_m)= P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)··P(w_m|w_1,...,w_{m-1})$

특히 초기 단어가 부여 된 이후 단어의 확률은 데이터로부터 추정하기가 어렵습니다. 이것은 왜 이 공동 확률(joint probability)이 i 번째 단어가 n-1 이전 단어에만 의존한다는 독립성 가정에 의해 근사화되는 이유입니다.

우리는 n-grams라고하는 n 개의 순차적 단어의 공동 확률 만 모델링합니다. n-gram은 길이가 n 인 다른 문자 (n 문자 등)를 나타 내기 위해 사용될 수 있습니다.

관절 분포의 추론은 다수의 독립된 부분에서 관절 분포를 나눈 n-gram 모델을 통해 근사화됩니다.

  n 그램은 여러 순차 단어의 조합입니다. 여기서 n은 순차적 단어의 수입니다. 예를 들어, 빠른 갈색 여우라는 구에는 다음과 같은 n 그램이 있습니다.

• 1-gram: "The," "quick," "brown," and "fox" (also known as unigram)
• 2-grams: "The quick," "quick brown," and "brown fox" (also known as bigram)
• 3-grams: "The quick brown" and "quick brown fox" (also known as trigram)
• 4-grams: "The quick brown fox"

이제 거대한 코퍼스(말뭉치,corpus)가 있으면 특정 n (일반적으로 2에서 4)까지 모든 n 그램을 찾아 그 코퍼스에서 각 n 그램의 발생을 계산할 수 있습니다.
이러한 카운트로부터, 이전의 n-1 워드가 주어지면 각 n 그램의 마지막 워드의 확률을 추정 할 수 있습니다.

1-gram: P(word) = $count(word)/total number of word in corpus$
2-gram: $P(W_i|W_{i-1}) = count(W_{i-1},W_i)/count(w_{i-1})
n-gram: $P((W_{n+i}|W_n....W_{n+i-1}) =count(W_n....W_{n+i-1},W_{n+1})/count(W_n....W_{n+i-1})$

i 번째 단어가 이전 n-1 단어에만 의존한다는 독립 가정은 이제 공동 분포를 근사화하는 데 사용될 수 있습니다. 예를 들어 유니그램의 경우 다음과 같이 합동 분포를 근사 할 수 있습니다.

$P(w_1....W_m)=P(W_1)P(W_2)...P(W_m)$

트리그램:

$P(w_1...w_m)= P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)··P(w_m|w_{m-2},w_{m-1})$

어휘 크기에 기반하여 n-gram의 수가 n과 함께 기하 급수적으로 증가 함을 알 수 있습니다.

예를 들어 작은 어휘에 100 단어가 포함 된 경우 가능한 5 그램의 수는 1005 = 10,000,000,000 개의 서로 다른 5 그램입니다. 비교해 보면 셰익스피어의 전체 작품에는 약 3 만 개의 서로 다른 단어가 들어있어 n 그램을 큰 n으로 사용하는 것이 불가능하다는 것을 보여줍니다.

모든 확률을 저장하는 문제가있을뿐만 아니라 더 큰 n 값에 대해 적절한 n-gram 확률 추정을 생성하기 위해 매우 큰 텍스트 코퍼스가 필요합니다.

 이 문제는 차원의 저주로 알려져 있습니다. 가능한 입력 변수 (단어)의 수가 증가하면이 입력 값의 다른 조합 수는 기하 급수적으로 증가합니다.

 이러한 차원의 저주는 학습 알고리즘이 n-gram 모델링의 경우와 관련하여 값의 관련 조합 당 적어도 하나의 예를 필요로 할 때 발생합니다.

우리 n이 크면 클수록 원래 분포를 더 잘 추정 할 수 있고 n-gram 확률을 잘 추정하는 데 더 많은 데이터가 필요합니다.

## neural language model

전 섹션에서는 n-gram으로 텍스트를 모델링 할 때 차원 성의 저주를 설명했습니다. 우리가 계산해야하는 n-gram의 수는 n과 어휘의 단어 수에 따라 기하 급수적으로 커집니다. 이 저주를 극복하는 한 가지 방법은 단어의 더 낮은 차원의 분산 표현을 학습하는 것입니다. 이 분산 표현은 다음과 같이 단어의 공간을 단어 삽입의 더 낮은 차원 공간으로 변환하는 포함 기능을 학습하여 작성됩니다.

![neural language](./deep_picture/neurallanguage.PNG)

어휘의 V- 단어는 크기 V의 1- 핫 인코딩 벡터로 변형됩니다 (각 단어는 고유하게 인코딩 됨). 그런 다음 임베딩 함수는이 V 차원 공간을 크기 D (여기서는 D = 4)의 분산 표현으로 변환합니다.

아이디어는 학습 된 임베딩 함수가 단어에 대한 의미 정보를 학습한다는 것입니다. 그것은 어휘의 각 단어를 연속적인 가치를 지닌 벡터 표현 인 단어 embedding과 연결시킵니다.

각 단어는이 단어의 문법적 또는 의미 론적 속성에 다른 차원이 해당되는이 단어의 포함 영역에 해당합니다.

목표는이 임베딩 영역에서 서로 가깝게 단어가 유사한 의미를 갖도록하는 것입니다. 이 방법으로, 일부 단어가 의미 론적으로 유사한 정보가 언어 모델에 의해 악용 될 수 있습니다.

예를 들어, "여우"와 "고양이"가 의미 상으로 관련되어 있고 "빠른 갈색 여우"와 "빠른 갈색 고양이"가 유효한 구문이라는 것을 알 수 있습니다.

그런 다음 일련의 단어를이 단어의 특성을 포착하는 일련의 포함 벡터로 변환 할 수 있습니다.

신경망을 통해 언어 모델을 모델링하고 암시 적으로이 내장 함수를 학습하는 것이 가능합니다. n-1 단어 $(w_{t-n + 1}, ..., w_{t-1})$의 시퀀스가 ​​다음 단어의 확률 분포, 즉 wt를 출력하려고하면 신경망을 배울 수 있습니다. 네트워크는 서로 다른 부분으로 구성됩니다.

임베딩 레이어는 단어 wi의 one-hot 표현을 취해 그것을 임베딩 행렬 C와 곱함으로써 임베딩으로 변환합니다.

이 계산은 테이블 조회를 통해 효율적으로 구현할 수 있습니다. 임베딩 매트릭스 C는 모든 단어에 대해 공유되므로 모든 단어는 동일한 포함 기능을 사용합니다. C는 V * D 매트릭스로 표현되며, 여기서 V는 어휘의 크기이고 D는 임베딩 크기입니다.

생성 된 삽입은 숨겨진 레이어로 연결됩니다. 그 후에, 바이어스 b, 비선형적 함수, tanh같은 것이 적용될 수있다.

숨겨진 계층의 출력은 함수 z = tanh (concat (wt-n + 1, ..., wt-1) + b)로 표현됩니다.

숨겨진 레이어에서 우리는 이제 숨겨진 레이어에 U를 곱하여 다음 단어 wt의 확률 분포를 출력 할 수 있습니다.
이것은 숨겨진 레이어를 단어 공간에 매핑하고 바이어스 b를 추가하고 softmax 함수를 적용하여 확률 분포를 얻습니다. 최종 계층은 softmax (z * U + b)를 계산합니다. 이 네트워크는 다음 그림과 같습니다.

![mapping](./deep_picture/mapping.PNG)

이 모델은 어휘의 모든 단어의 임베딩과 단어의 연속에 대한 확률 함수의 모델을 동시에 습득합니다.
이 확률 함수를 이러한 분산 된 표현 덕분에 훈련 중에 보지 못한 일련의 단어로 일반화 할 수 있습니다. 테스트 세트의 특정 단어 조합은 트레이닝 세트에서 볼 수 없지만 유사한 임베딩 기능이있는 시퀀스는 트레이닝 중에 훨씬 더 많이 보입니다.

다음 그림은 일부 단어 포함에 대한 2D 투영을 보여줍니다. 의미 적으로 가까운 단어가 임베딩 공간에서 서로 가깝다는 것을 알 수 있습니다.

![워드백터](./deep_picture/wordvec.PNG)

단어 임베딩은 텍스트 데이터의 큰 코퍼스에서 자율 학습을 할 수 있다. 이렇게하면 단어 사이에 일반적인 의미 정보를 포착 할 수 있다.
그 결과 생성 된 임베딩을 사용하여 많은 레이블 데이터가 사용 가능하지 않은 다른 태스크의 성능을 향상시킬 수 있습니다.

단어 삽입은 텍스트 데이터의 큰 코퍼스에서 자율 학습을 할 수 있습니다. 이렇게하면 단어 사이에 일반적인 의미 정보를 포착 할 수 있습니다.

그 결과 생성 된 임베딩을 사용하여 많은 레이블 데이터가 사용 가능하지 않은 다른 태스크의 성능을 향상시킬 수 있습니다.

예를 들어, 기사의 정서를 분류하려고하는 분류자는 one-hot 인코딩 벡터 대신에 이전에 습득 한 단어 삽입을 사용하여 훈련 될 수 있습니다.

이 방법으로 단어의 의미 정보가 감정 분류 자에게 즉시 제공됩니다. 이 때문에, 많은 연구가 단어의 연속에 대한 확률 함수를 학습하는 것에 초점을 두지 않고 더 나은 단어 삽입을 만들었습니다. 예를 들어 인기있는 단어 삽입 모델은 word2vec입니다.
놀라운 결과는 이러한 단어 삽입이 단어 들간 유추를 차이로 포착 할 수 있다는 것입니다. 예를 들어, "여성"과 "남자"의 삽입 사이의 차이가 성을 인코딩하고이 차이는 "여왕"및 "왕"과 같은 다른 성 관련 단어에서 동일하다는 것을 알 수 있습니다.

![워드2백터](./deep_picture/word2vec.PNG)

이전의 피드 포워드 네트워크 언어 모델은 큰 어휘 입력을 모델링하는 차원의 저주를 극복 할 수 있지만 여전히 고정 길이 단어 시퀀스 만 모델링하는 것으로 제한됩니다.

이 문제를 극복하기 위해 RNN을 사용하여 고정 길이 단어 시퀀스에 제한되지 않는 RNN 언어 모델을 작성할 수 있습니다.

  이러한 RNN 기반 모델은 입력 임베딩에서 유사한 단어를 클러스터링 할 수있을뿐만 아니라 반복적 인 상태 벡터에서 유사한 히스토리를 클러스터링 할 수 있습니다.

이러한 단어 기반 모델의 한 가지 문제점은 어휘의 각 단어에 대한 출력 확률 P (wi | context)를 계산하는 것입니다.
모든 단어 활성화에 대해 softmax를 사용하여 이러한 출력 확률을 얻습니다.

  50,000 단어의 작은 어휘 V의 경우, |S|*|V| 출력 매트릭스가 필요하다 여기서 V는 어휘의 크기이며 S는 상태벡터의 크기.

softmax는 다른 모든 활성화를 조합하여 단일 단어의 활성화를 정상화하므로 단일 단어의 확률을 얻기 위해 각 활성화를 계산해야합니다.

둘 모두 큰 어휘에서 softmax를 계산하는 데 따른 어려움을 보여줍니다. softmax 이전에 선형 변환을 모델링하기 위해 많은 매개 변수가 필요하며 softmax 자체는 계산 집약적입니다.

이 문제를 극복 할 수있는 방법이 있습니다. 예를 들어 softmax 함수를 이진 트리로 모델링하여 log (| V |) 계산 만하면 단일 단어의 최종 출력 확률을 계산할 수 있습니다.

이러한 해결 방법을 자세히 살펴 보지 않고 이러한 큰 어휘 문제의 영향을받지 않는 언어 모델링의 또 다른 변종을 확인해보십시오.

## Character-based model

대부분의 경우, 언어 모델링은 단어 레벨에서 수행되며, 여기서 분포는 | V | 단어의 고정 된 어휘입니다.

음성 인식에 사용되는 언어 모델과 같은 사실적인 작업의 어휘는 종종 10 만 단어를 초과합니다.

  이러한 거대한 차원은 출력 분포를 모델링하는 것을 매우 어렵게 만듭니다.

또한 이러한 단어 수준 모델은 다 자릿수 또는 훈련 데이터의 일부가 아닌 단어 (단어 외)와 같은 단어가 아닌 문자열을 포함하는 텍스트 데이터를 모델링 할 때 상당히 제한적입니다.

이러한 문제를 극복 할 수있는 모델 클래스를 문자 수준 언어 모델이라고합니다.

  이 모델은 단어 대신 문자의 시퀀스에 대한 분포를 모델링하므로 훨씬 작은 어휘에 대한 확률을 계산할 수 있습니다.

여기서 말하는 어휘는 텍스트 코퍼스의 가능한 모든 문자로 구성됩니다. 그러나 이러한 모델에는 단점이 있습니다.

단어가 아닌 문자 시퀀스를 모델링하면 시간이 지남에 따라 동일한 정보를 캡처하기 위해 더 긴 시퀀스를 모델링해야합니다. 이러한 장기 의존성을 포착하기 위해 LSTM RNN 언어 모델을 사용합시다.

이 섹션의 다음 부분에서는 Tensorflow에서 캐릭터 레벨 LSTM을 구현하는 방법과 Leo Tolstoy의 전쟁과 평화에서이를 훈련시키는 방법에 대해 자세히 설명합니다.

이 LSTM은 P (ct | ct-1 ... ct-n) 문자를 통해 다음 문자의 확률을 모델링합니다. 전체 텍스트가 너무 길어 BPTT (back-propagation through time)가있는 네트워크를 학습 할 수 없으므로 truncated(잘린) BPTT라는 일괄 처리 된 변형을 사용할 것입니다

이 방법에서는 트레이닝 데이터를 고정 된 시퀀스 길이의 일괄 처리로 나누고 배치별로 네트워크 일괄 처리를 수행합니다.
일괄 처리가 서로 후속 작업을 수행하므로 마지막 일괄 처리의 마지막 상태를 다음 일괄 처리의 초기 상태로 사용할 수 있습니다.

이렇게하면 전체 입력 텍스트를 통해 전체 역 전파를 수행하지 않고도 상태에 저장된 정보를 이용할 수 있습니다. 다음에는 이러한 배치를 읽고이를 네트워크에 공급하는 방법을 설명합니다.

## Preprocessing and reading data

좋은 언어 모델을 훈련 시키려면 많은 양의 데이터가 필요합니다. 예를 들어 레오 톨스토이 (Leo Tolstoy)의 "전쟁과 평화"에 대한 영어 번역본을 기반으로 한 모델에 대해 배우게됩니다.

이 책은 50 만 단어 이상을 포함하고있어 우리의 작은 예를위한 완벽한 후보자입니다.

그것이 공개 도메인에 있기 때문에 "전쟁과 평화"는 Project Gutenberg에서 무료로 일반 텍스트로 다운로드 할 수 있습니다.

사전 처리의 일환으로 Gutenberg 라이센스, 서적 정보 및 목차를 제거합니다. 다음 문장의 중간에 줄 바꿈을 제거하고 최대 연속 줄 수를 2 줄로 줄입니다.

데이터를 네트워크에 공급하려면 숫자 형식으로 변환해야합니다. 각 문자는 정수와 연결됩니다.

이 예에서는 텍스트 코퍼스에서 총 98 개의 다른 문자를 추출합니다.

  다음으로 입력과 목표를 추출합니다. 각 입력 문자에 대해 다음 문자를 예측합니다. 우리는 잘린 BPTT로 훈련하기 때문에, 시퀀스의 연속성을 이용하기 위해 모든 배치를 서로 후속 조치 할 것입니다.

  텍스트를 인덱스 목록으로 변환하고 입력 및 대상의 일괄 처리로 분할하는 프로세스가 다음 그림에 설명되어 있습니다.

![프로세싱](./deep_picture/prosessing.PNG)


## LSTM network

교육 할 네트워크는 각 레이어에 512 개의 셀이있는 2 계층 LSTM 네트워크입니다.
절단 된 BPTT로이 네트워크를 교육 할 것이므로 배치간에 상태를 저장해야합니다.
먼저, 입력 및 대상에 대한 자리 표시자를 정의해야합니다.
입력과 목표의 첫 번째 차원은 배치 크기, 병렬로 처리되는 예제의 수입니다.
두 번째 차원은 텍스트 시퀀스의 차원입니다.
이 두 자리 표시자는 문자가 인덱스로 표시되는 일련의 일괄 처리를 사용합니다.

inputs = tf.placeholder(tf.int32, (batch_size, sequence_length))
targets = tf.placeholder(tf.int32, (batch_size, sequence_length))

문자를 네트워크에 공급하려면 문자를 벡터로 변환해야합니다.

우리는 그것들을 one-hot 인코딩으로 변환 할 것입니다. 즉, 각 캐릭터는 데이터 세트의 다른 문자 수와 동일한 길이의 벡터로 변형됩니다. 이 벡터는 인덱스에 해당하는 셀을 제외하고 모두 0입니다.
이 값은 1로 설정됩니다. 다음 코드 줄을 사용하여 TensorFlow에서 쉽게 수행 할 수 있습니다.

one_hot_inputs = tf.one_hot(inputs, depth=number_of_characters)

다음으로, 우리는 우리의 다층 LSTM 아키텍처를 정의 할 것입니다. 먼저 각 레이어에 대해 LSTM 셀을 정의해야합니다 (lstm_sizes는 각 레이어의 크기 목록입니다 (예 : (512, 512)).

cell_list = (tf.nn.rnn_cell.LSTMCell(lstm_size) for lstm_size in lstm_ sizes)

그런 다음이 셀을 다음을 사용하여 단일 다층 RNN 셀에 래핑합니다.

multi_cell_lstm = tf.nn.rnn_cell.MultiRNNCell(cell_list)

일괄 처리간에 상태를 저장하려면 네트워크의 초기 상태를 가져 와서 저장할 변수에 래핑해야합니다.

계산상의 이유로 TensorFlow는 LSTM 상태를 두 개의 개별 텐서 (튜너의 긴 기간 메모리 섹션의 c 및 h)로 구성된 튜플에 저장합니다.

이 중첩 된 데이터 구조를 flatten 메소드로 전개하고, 각 텐서를 변수에 랩핑 한 다음 pack_sequence_as 메소드로 원래 구조로 다시 패키징 할 수 있습니다.

initial_state = self.multi_cell_lstm.zero_state(batch_size, tf.float32)
state_variables = tf.python.util.nest.pack_sequence_as(    self.initial_state,    (tf.Variable(var, trainable=False)      for var in tf.python.util.nest.flatten(initial_state)))

이제 초기 상태를 변수로 정의 했으므로 시간 경과에 따라 네트워크 풀기를 시작할 수 있습니다.

TensorFlow는 입력의 시퀀스 길이에 따라이 언 롤링을 동적으로 수행하는 dynamic_rnn 메서드를 제공합니다. 이 메소드는 LSTM 출력을 나타내는 텐서와 최종 상태로 구성된 튜플을 반환합니다.

lstm_output, final_state = tf.nn.dynamic_rnn(    cell=multi_cell_lstm, inputs=one_hot_input, initial_state=state_variable)

다음으로, 최종 상태를 다음 배치의 초기 상태로 저장해야합니다.
우리는 변수 assign 메소드를 사용하여 각각의 최종 상태를 올바른 초기 상태 변수에 저장합니다.
control_dependencies 메서드는 LSTM 출력을 반환하기 전에 상태 업데이트가 실행되도록 강제하는 데 사용됩니다.

store_states = (    state_variable.assign(new_state)    for (state_variable, new_state) in zip(        tf.python.util.nest.flatten(self.state_variables),        tf.python.util.nest.flatten(final_state))) with tf.control_dependencies(store_states):    lstm_output = tf.identity(lstm_output)

최종 LSTM 출력에서 logit 출력을 얻으려면 일괄 크기(batch size) * 시퀀스 길이(sequence length) * 심볼 수(number of symbol)를 차원으로 가질 수 있도록 선형 변환을 출력에 적용해야합니다.

이 선형변환을 적용하기 전에 출력을 아웃풋*아웃풋함수 개수의 행렬로 만들 필요가 잇음.

output_flat = tf.reshape(lstm_output, (-1, lstm_sizes(-1)))
그런 다음 우리는 가중치 행렬 W와 바이어스 b를 사용하여 선형 변환을 정의하고 적용하여 로짓을 얻고, softmax 함수를 적용하고,
배치사이즈*시퀀스길이*문자수 의 텐서로 재구성한다.

~~~python

# Define output layer
logit_weights = tf.Variable(
    tf.truncated_normal((lstm_sizes(-1), number_of_characters), stddev=0.01))
logit_bias = tf.Variable(tf.zeros((number_of_characters))) 
# Apply last layer transformation
logits_flat = tf.matmul(output_flat, self.logit_weights) + self.logit_ bias
probabilities_flat = tf.nn.softmax(logits_flat)
# Reshape to original batch and sequence length 
probabilities = tf.reshape(probabilities_flat, (batch_size, -1, number_of_characters))

~~~

![LSTm](./deep_picture/LSTM_model.PNG)
