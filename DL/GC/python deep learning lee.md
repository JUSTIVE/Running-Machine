#h1 MASHIN LERANING

기계 학습 기술은 실제로 다양한 분야에 적용되고 있으며 데이터 과학자는 여러 산업 분야에서 연구되고 있습니다.

기계 학습을 통해 의사 결정을 내릴 수 있도록 데이터에서 쉽게 알 수 없는 지식을 얻는 프로세스를 식별합니다.

기계 학습 기술의 응용은 크게 다를 수 있으며 의학, 금융 및 광고와 같은 다양한 분야에 적용 할 수 있습니다.

이 장에서는 다양한 기계 학습 접근법과 기술 및 실제 문제에 대한 응용 프로그램을 소개하고 기계 학습,  Python에서 사용할 수있는 주요 오픈 소스 패키지 중 하나를 소개합니다.scikit-learn

이것은 우리가 뇌의 기능성, 특히 DEEP learning을 목표로 하는 신경 네트워크를 사용하는 특정한 유형의 기계 학습 접근법에 초점을 맞추는 차후의 장의 배경지식을 제공 할 것이다.

deep learing은 80 년대에 사용 된 것보다 더 진보 된 신경 네트워크를 사용합니다. 이론의 최근 발전뿐만 아니라 컴퓨터 속도의 향상과 GPU (Graphical Processing Units)의 사용 대 CPU의 전통적인 사용 ( 컴퓨팅 처리 장치)에도 기인함.

이 장은 주로 기계 학습이 할 수있는 것과 수행 할 수있는 것의 요약을 의미하며 독자가 알려진 전통적인 머신러닝과 차별화 된 학습 방법을 더 잘 이해할 수 있도록 독자를 준비시키기위한 것입니다.

--
특히, 이 챕터에서는 다음과 같은 내용을 다룹니다. 

• 기계 학습이란 무엇입니까? 
• 다양한 기계 학습 접근 방식 
• 기계 학습 시스템에 관련된 단계 
• 인기 있는 기술/알고리즘에 대한 간략한 설명 
• 실제 삶에 적용.
• 알려진 오픈 소스 패키지

#h2 왜 머신러닝을 배우지?

기계 학습은 종종 "빅 데이터"와 "인공지능" 또는 짧게는 A.I.와 같은 용어와 함께 언급되지만, 둘 다와는 상당히 다릅니다. 

기계 학습이 무엇이고 왜 유용한지 이해하기 위해서는 빅 데이터가 무엇이고 어떻게 기계 학습이 여기에 적용되는지를 이해하는 것이 중요합니다.

빅 데이터는 수집 및 저장되는 데이터의 큰 증가로 인해 생성되는 대규모 데이터 세트를 설명하는 데 사용되는 용어입니다.

구글은 혼자 하루에 20페타바이트 이상의 정보를 처리하는 것으로 추정되고 있으며 이 숫자는 증가할 것이다. 

IBM은 매일 250조 바이트가 생성되고 전세계 데이터의 90%가 지난 2년 동안 생성되었다고 추정했습니다. 

분명히 인간만으로는 엄청난 양의 데이터를 분석 할 수는 없지만 기계 학습 기술은 이러한 매우 큰 데이터 세트를 이해하는 데 사용됩니다

많은 머신러닝과 특히 딥러닝의 장점중 하나는 분석 및 예측 능력을 향상시키는 대규모 데이터 세트에 사용할 수 있을 때 가장 잘 작동한다는 것입니다. 

다른 말로는, 머신러닝, 특히 딥러닝 신경 네트워크는 데이터에 숨겨진 패턴과 정규성을 발견하기 위해 대용량 데이터 세트에 액세스할 수 있을 때 "학습"을 가장 잘 합니다.

---

반면에 기계 학습의 예측 능력은 인공 지능 시스템에 잘 적용될 수 있습니다.

기계 학습은 인공지능 시스템의 "두뇌"라고 생각할 수 있다. 인공 지능은 (이 정의가 독특하지는 않더라도) 환경과 상호작용할 수 있는 시스템으로 정의할 수 있다. 

인공 지능 기계에는 센서가 장착되어있어 자신이 속한 환경과 관련 지을 수있는 도구에 대해 알 수 있다.

따라서 기계 학습은 기계가 센서를 통해 섭취한 데이터를 분석하여 적절한 답을 만들 수 있도록 하는 두뇌입니다.

간단한 예는 아이폰의 시리이다. 시리는 마이크를 통해 명령을 듣고 스피커나 디스플레이를 통해 답을 출력하지만, 그렇게 하기 위해서는 올바른 대답을 만들기 위해 무엇을 "인식"해야 합니다. 

마찬가지로, 운전자 없는 자동차에는 카메라, GPS 시스템, 수중 음파탐지기 레이더가 장착될 것이지만, 이 모든 정보는 정확한 답을 제공하기 위해 처리되어야 한다. 즉, 가속, 제동, 회전 등. 답을 이끌어내는 정보처리는 기계학습의 의미를 나타낸다.

#h2 다양한 기계 학습 접근 방식

기계 학습이라는 용어는 우리가 본 것처럼 매우 일반적인 방법으로 사용되며, 그것은 큰 집합에서 패턴을 추론하거나 사용 가능한 데이터를 분석하여 학습된 것을 바탕으로 새로운 데이터에 대한 예측을 하는 일반적인 기술을 가리킨다. 

이것은 매우 일반적이며 광범위한 정의이며 다양한 기술을 포함합니다. 기계 학습 기술은 크게 두 가지 큰 클래스로 나눌 수 있습니다. 감독 및 감독되지 않은 학습(Supervised ad unsupervised learning)입니다. 하나 이상의 클래스가 종종 추가되며 강화 학습(Reinforcement learning)이라고합니다.

#h2 지도 학습.

기계 알고리즘의 첫 번째 클래스는 지도 학습으로 지정됩니다. 

감독(지도) 학습 알고리즘은 라벨링되지 않은 유사한 데이터를 분류하기 위해 라벨로 표시된 데이터 세트를 사용하는 기계 학습 알고리즘의 한 클래스입니다.

라벨화된 데이터는 이미 분류된 데이터이며 라벨화되지 않은 데이터는 아직 라벨링되지 않은 데이터입니다. 우리가 보게 될 것처럼 라벨은 불연속적이거나 연속적일 수 있다. 이 개념을 더 잘 이해하기 위해서, 예를 들어보자

사용자가 매일 많은 양의 전자 메일을 수신한다고 가정합니다. 그 중 일부는 중요한 비즈니스 전자 메일이고 일부는 요청되지 않은 정크 전자 메일 또는 스팸입니다.

감독되는 기계 알고리즘에는 사용자가 이미 스팸 또는 스팸이 아닌 스팸이라는 레이블을 지정한 대규모 전자 메일이 표시됩니다.

알고리즘은 모든 분류 된 데이터에 대해 실행되며 전자 메일이 스팸인지 아닌지에 대한 예측을합니다.

즉, 알고리즘이 각 예를 검사하고 전자 메일이 스팸인지 여부에 대해 각 예제에 대해 예측합니다. 

일반적으로 알고리즘이 레이블이 지정되지 않은 모든 데이터를 처음 실행할 때 많은 전자 메일에 레이블을 잘못 지정하며 성능이 상당히 저하될 수 있습니다. 그러나 각 실행 후 알고리즘은 예측값을 원하는 결과(레이블)와 비교합니다. 이를 통해 알고리즘은 성능과 정확성을 향상시키는 방법을 배우게 됩니다. 

위에서 언급했듯이, 이러한 종류의 접근 방식은 각 전자 메일이 스팸으로 분류 될 수있는 특성 (또는 기능)을 더 잘 학습 할 수있는 많은 양의 데이터로부터 이점을 얻을 수 있습니다.

알고리즘이 라벨 데이터 (흔히 훈련 데이터라고도 함)에서 잠시 동안 실행되고 정확도가 향상되지 않으면 새로운 전자 메일에서 새 레이블이 지정되지 않은 데이터의 정확성을 테스트 할 수 있습니다

그러나 이 `프로세스를 두 개 이상의 클래스로 일반화할 수 있다는 점을 유념`해야 합니다. 예를 들어, 소프트웨어를 실행하고 라벨이 개인, 비즈니스/워크, 사회, 혹은 스팸 레이블이 있는 전자 메일 세트에서 교육할 수 있습니다.

실제로 Google의 무료 전자 메일 서비스 인 Gmail은 사용자가 다음과 같은 다섯 가지 범주를 선택할 수 있도록합니다.

• 주, 개인 대 대화를 포함합니다.
• 소셜, 소셜 네트워크 및 미디어 공유 사이트의 메시지 포함
• 프로모션,마케팅 전자 메일, 쿠폰 및 할인을 포함.
• 업데이트, 은행 계좌 명세서 및 영수증을 포함한 청구서
• 포럼,온라인 그룹 및 메일 링리스트의 메시지가 포함

경우에 따라 결과가 반드시 개별적 일 필요는 없으며 데이터를 분류 할 수있는 클래스 수가 제한되어 있지 않을 수도 있습니다.

예를 들어, 우리는 사전 결정된 건강 매개 변수를 기반으로 한 집단의 평균 수명을 예측하려고 시도했을 수 있습니다. 이 경우 결과는 연속 함수이기 때문에 (우리는 기대 수명을 사람이 살 것으로 예상되는 실제 숫자로 지정할 수 있습니다) 우리는 분류 작업에 대해 이야기하지 않고 회귀 문제에 대해서만 이야기합니다.

감독 학습을 생각하는 한 가지 방법은 데이터 집합에 정의 된 함수 f를 작성하려고한다는 것입니다.

우리의 데이터 세트는 기능별로 구성된 정보로 구성됩니다. 전자 메일 분류의 예에서 이러한 기능은 스팸 전자 메일에서 다른 기능보다 자주 나타날 수있는 특정 단어 일 수 있습니다.

노골적인 성 관련 단어를 사용하면 비즈니스 / 업무용 전자 메일이 아닌 스팸 전자 메일을 식별 할 가능성이 높습니다. 반대로 "회의", "비즈니스"및 "프레젠테이션"과 같은 단어는 작업 전자 메일을보다 쉽게 설명합니다. 메타 데이터에 대한 액세스 권한이 있으면 보낸 사람 정보를 사용하여 전자 메일을 더 잘 분류 할 수도 있습니다.

각 전자 메일에는 일련의 기능이 관련되며 각 기능에는 값이 있습니다(이 경우 전자 메일 본문에 특정 단어가 있는 횟수). 

그런 다음 기계 학습 알고리즘은 이러한 값을 클래스 집합을 나타내는 이산형 범위 또는 회귀의 경우 실제 값에 매핑하려고 합니다. 이 알고리즘은 레이블이 지정된 대부분의 데이터와 정확히 일치할 수 있는 최상의 기능을 정의할 수 있을 때까지 많은 예제에서 실행됩니다. 그런 다음 레이블이 지정되지 않은 데이터를 실행하여 사람의 개입 없이 예측합니다. 다음과 같은 기능을 정의합니다.

그런 다음 기계 학습 알고리즘은 이러한 값을 클래스 집합을 나타내는 불연속 범위 또는 회귀의 경우 실제 값에 매핑하려고 시도합니다.

알고리즘은 대부분의 레이블이 지정된 데이터를 올바르게 일치시킬 수있는 최상의 기능을 정의 할 수있을 때까지 많은 예제로 실행됩니다. 그런 다음 사용자 개입없이 예측을하기 위해 레이블이 지정되지 않은 데이터를 통해 실행할 수 있습니다.

이것은 함수를 정의한다:

f:space of featurees -> classes =( discreate values or real values)

우리는 또한 분류를 다른 데이터 포인트 그룹을 분리하려는 과정(process)으로 생각할 수 있다

 기능을 정의하고 나면 데이터 세트의 전자 메일과 같은 예는 기능의 공간에서 각 지점이 다른 예 (또는 전자 메일)를 나타내는 지점으로 생각할 수 있습니다.

기계 알고리즘 작업은 비 스팸 전자 메일과 스팸을 분리하는 것과 같은 방식으로 다른 특성을 가진 지점을 분리하는 하이퍼 평면-hyper-plane (즉, 고차원 공간의 평면)을 그리는 것입니다.

아래 그림과 같이 2 차원의 경우에는 사소한 것처럼 보일 수 있지만 수백 또는 수천 차원의 예제에서는 매우 복잡 할 수 있습니다.

[선형회귀 1차]](C:\users\dlramcjf\Desktop\python\one plane.PNG)

이후 장에서 분류 또는 회귀 문제의 몇 가지 예를 살펴보겠습니다. 

이러한 문제 중 하나는 자릿수 분류입니다. 0부터 9까지를 나타내는 이미지 세트를 고려할 때 기계 학습 알고리즘은 각 이미지를 지정하는 숫자를 분류하려고 시도합니다. 

이러한 예에서, 우리는 가장 고전적인 데이터셋 중 하나인 MNIST 데이터셋을 사용할 것이다. 이 예에서는 각 자릿수가 28 x 28(=784) 픽셀의 영상으로 표시되며, 10자리 각각을 분류해야 하므로 784차원 공간에 9개의 분리된 하이퍼 평면을 그려야 합니다.

#h2 감독되지 않은 학습.

기계 학습 알고리즘의 두 번째 클래스는 감독되지 않은 학습(자율학습)이라고합니다. 이 경우 사전에 데이터에 레이블을 지정하지 않고 알고리즘을 결론 짓게합니다.

자율 학습의 가장 보편적이고 가장 단순한 예 중 하나는 클러스터링입니다.
이것은 데이터를 부분 집합으로 분리하려고 시도하는 기술입니다.

예를 들어 이전의 스팸/스팸이 아닌 메일의 경우 알고리즘은 모든 스팸 전자 메일에 공통되는 요소(예: 철자가 틀린 단어가 있음)를 찾을 수 있습니다.

이것이 무작위 분류보다 더 나은 분류를 제공할 수 있지만 스팸/비 스팸 메일이 그렇게 쉽게 분리될 수 있다는 것은 분명하지 않다. 

알고리즘이 데이터를 분리하는 부분 집합은 데이터 집합의 다른 클래스입니다. 클러스터링이 작동하려면 각 클러스터의 각 요소가 원칙적으로 클래스 내 유사성이 높고 다른 클래스와의 유사성이 낮아야합니다.

Clustering may work with any number of classes, and the idea behind clustering methods such as k-means is to find k-subsets of the original data whose elements are closer (more similar) to each other than they are to any other element outside their class. 

클러스터링은 원하는 수의 클래스에서 작동할 수 있으며, k-mean 과 같은
클러스터링 방법 개념은 다른 클래스보다 요소가 서로 더 가까운 원본 데이터의 k-subset을 찾는 것입니다.

물론, 이렇게 하기 위해서는 어떤 더 가깝거나 더 비슷한 의미를 정의할 필요가 있습니다. 즉, 포인트 사이의 거리를 정의하는 일종의 측정 기준을 정의해야 합니다.(예를 들어 선형관계..?)

--


[유한집합 클러스터링](C:\Users]dlrmacjf\Desktop\python\limit cluster.PNG)

[무한집합 클러스터링](C:\Users\dlrmacjf\Desktop\python\unlimit cluster.PNG)

클러스터링만이 자율학습이 아니며, 우리는 딥러닝의 최근 성공이 그것이 자율학습 테스크에서 매우 효과적이게 되는 것과 관련이 있다는 것을 알게 될 것이다.

새로운 데이터는 매우 빠르게 매일 생성되며, 모든 새 데이터에 라벨을 붙이는 작업은 상당히 힘들고 시간이 많이 소요됩니다.

자율학습 알고리즘의 장점 중 하나는 라벨이 붙은 데이터가 필요하지 않다는 것이다. 제한된 볼트만 머신(RBM)와 같은 감독되지 않은 심층 학습 기법 및 방법은 데이터에서 기능을 추출하여 작동합니다.

예를 들어, MNIST 데이터 세트를 사용하여 제한된 볼트만 시스템은 각 자릿수에 고유한 특성을 추상화하여 각 자릿수의 선과 원곡선의 모양을 탐지합니다.

자율학습은 데이터를 레이블에 매칭하는 것이 아니라 그에 따라 분류할 수 있는 숨겨진 구조를 드러냄으로써 작동합니다

또한, 예를 들어, 깊은 믿음의 그물을 가지고, 우리는 자율학습의 성능을 감독학습으로 정제함으로써 개선할 수 있다.

** 추가적인 검색으로 인한 자료

이때 RBM이란.확률적으로 순환하는 신경망 네트워크이다.

[볼츠만 머신](C:\Users\dlrmacjf\Desktop\python\Boltman.PNG)
볼츠만 머신(Boltzmann machine)은 볼 수 있는 층(visible layer)과 숨겨진 층(hidden layer)의 두 층으로 구성된 그래픽 모델이다. 그런데 이 모델에서 각 볼 수 있는 유닛은 숨겨진 유닛들과만 연결되고, 볼 수 있는 유닛들 사이에 그리고 숨겨진 유닛들 사이에는 서로 연결이 없을 때 이를 제한된 볼츠만 머신(RBM: restricted Boltzmann machine)이라 한다

**

#h2 강화학습(Reinforcement learning )

기계 학습 기술의 세 번째 클래스는 강화 학습이라고합니다.

이는 성능 향상을 위해 피드백 요소를 사용하지만 감독학습과는 다르게 작동합니다. 

강화 학습 기법의 일반적인 적용은 게임을 하는 방법을 기계들에게 가르치는 것이다: 이 경우, 우리는 각각의 움직임을 좋거나 나쁘다고 지칭하지 않지만, 피드백은 게임의 결과나 게임 중 신호를 통해 이루어진다, 득점이나 실점과 같은

게임에서 이기는 것은 올바른 숫자를 인식하거나 전자 메일이 스팸인지 아닌지와 같은 긍정적인 결과를 반영하는 반면, 게임에서 진다면 더 많은 "학습"을 필요로 할 것이다. 

강화 학습 알고리즘은 게임에서 승리하는 것과 같은 성공적인 결과를 가져온 과거에 시도 된 행동을 재사용하는 경향이 있습니다. 그러나 미지의 영역에서 알고리즘은 결과에 따라 게임의 구조를 더 깊이 배울 수있는 새로운 동작을 시도해야합니다.

일반적으로 행동은 상호 관련이 있기 때문에 "좋음"또는 "나쁨"으로 평가 될 수있는 단일 행동이 아니라 함께 평가되는 행동의 전체적 역학이다.

Similar to how in playing chess sometimes sacrificing a pawn may be considered a positive action if it brings a better positioning on the chessboard, even though the loss of a piece is, in general, a negative outcome, in reinforcement learning it is the whole problem and its goal that is explored.

체스를 하는 것과 비슷하다. 때로는 폰을 희생시키는 것이 일반적으로 부정적인 결과인데도 체스판에서 더 나은 위치를 가져오는 것 처럼. 비록 한 조각의 손실이 일반적으로 강화 학습에서 부정적인 결과이지만 더 나은 위치를 가져오면 긍정적인 행동으로 간주될 수 있습니다.??

예를 들어, 움직이는 청소 로봇은 방을 계속 청소할지 또는 충전소로 다시 이동 할지를 결정해야 할 수도 있습니다.

그러한 결정은 유사한 상황에서 배터리가 소진되기 전에 충전소를 찾을 수 있었는지 여부에 기초하여 이루어질 수있다.

강화 학습에서 기본 아이디어는 최종적으로 얻는 보상이다. 알고리즘은 알고리즘이받는 총 보상을 최대화하기 위해 노력합니다.

보강 학습의 간단한 예제를 사용하여 tic-tac-toe의 고전 게임을 즐길 수 있습니다. 이 경우 보드의 각 위치는 이전 경험을 토대로 그 주에서 게임을 승리 할 확률 인 확률 (값)과 관련이 있습니다.

처음에는 각 위치의 승률 상태가 50 %로 설정되어 있습니다. 이는 처음에는 어느 위치에서나 승리 할 확률이 동일하다고 가정합니다.

일반적으로 기계는 게임에서 이기기 위해 더 높은 값을 가진 위치로 이동하려고 시도하고, 대신에 잃는다면 다시 평가할 것입니다. 각 위치에서 기계는 고정 된 결정된 규칙보다는 가능한 결과에 따라 선택합니다. 계속해서 플레이 할 때,이 확률은 정교 해지고 위치에 따라 성공 확률이 더 높거나 낮을 것입니다.

--

#h2 기계 학습 시스템에 관련된 단계

지금까지 우리는 서로 다른 기계 학습 접근 방식에 대해 논의했으며, 세 가지 다른 클래스로 대략 정리했습니다.

고전 기계 학습의 또 다른 중요한 측면은 당면의 문제를 더 잘 이해할 수 있도록 데이터를 이해하는 것입니다. 기계 학습을 적용하기 위해 정의해야 할 중요한 측면은 대략 다음과 같이 설명 할 수 있습니다.

• 학습자 : 이것은 사용되는 알고리즘과 "학습 철학"을 나타냅니다. 다음 단락에서 볼 수 있듯이 서로 다른 학습 문제에 적용 할 수있는 여러 가지 다양한 기계 학습 기술이 있습니다. 여러 가지 문제가 특정 기계 학습 알고리즘에 더 적합 할 수 있으므로 학습자를 선택하는 것이 중요합니다.
• 트레이닝 데이터 : 우리가 관심을 갖고있는 원시 데이터 세트입니다. 이러한 데이터는 자율 학습을 위해 레이블이 지정되지 않거나 감독 학습을위한 레이블을 포함 할 수 있습니다. 학습자가 문제의 구조를 이해할 수 있도록 충분한 샘플 데이터를 갖는 것이 중요합니다.

• 표현: 학습자가 데이터를 수집할 수 있도록 선택한 형상의 측면에서 데이터를 표현하는 방법입니다. 예를 들어 이미지를 사용하여 숫자를 분류하려는 경우 이는 이미지의 픽셀을 설명하는 값의 배열을 나타냅니다. 더 나은 결과를 얻기 위해서는 데이터 표현을 잘 선택하는 것이 중요하다.

• 목표: 이는 당면한 문제에 대한 데이터에서 배워야 하는 이유를 나타냅니다. 이는 대상과 엄격히 관련되어 있으며 학습자가 어떤 방법을 사용해야 하는지, 어떤 표현을 사용해야 하는지 정의하는 데 도움이 됩니다. 예를 들어, 원치 않는 이메일의 우편함을 정리하는 것이 목표일 수 있으며, 목표는 스팸 메일 탐지 등과 같은 학습자의 목표가 무엇인지 정의합니다.

•대상: 학습 대상과 최종 출력을 나타냅니다. 라벨링되지 않은 데이터의 분류일수 있으며, 숨겨진 패턴이나 특성에 따라 입력 데이터를 나타낼 수 있으며,향후 예측을 위한 시뮬레이터일 수 있으며, 외부 자극에 대한 반응일 수 있다. 
강화 학습의 경우 전략이 될 수 있습니다.

어떠한 기계 학습 알고리즘도 완벽한 수치 설명이 아니라 대상의 근사치만 달성할 수 있다. 이것은 아무리 강조해도 지나치지 않습니다.

기계 학습 알고리즘은 문제에 대한 정확한 수학적 해결책이 아니라 단지 근사치일 뿐이다.

이전 단락에서 우리는 학습이 특징(입력)의 공간에서부터 다양한 범위의 클래스로 정의했습니다.

우리는 나중에 신경 네트워크와 같은 특정 기계 학습 알고리즘이 이론적으로 어느 정도의 기능을 근사시킬 수있는 것으로 입증 될 수 있음을 알게 될 것입니다.

이 정리를 보편적 근사 이론(Universal Approximation Theorem)이라고 부르지 만, 문제에 대한 정확한 해결책을 얻을 수 있다는 것을 의미하지는 않습니다. 또한 문제에 대한 해결책은 트레이닝 데이터를 더 잘 이해함으로써 더 잘 달성 될 수 있습니다.

일반적으로 고전적인 기계 학습 기술로 해결할 수있는 문제는 전개 전에 교육 데이터를 철저하게 이해하고 청소해야 할 수 있습니다.기계 학습 문제에 접근하는 데 필요한 몇 가지 단계를 설명해야 하는 경우 다음과 같이 요약할 수 있습니다.

• 데이터 수집 : 가능한 한 많은 데이터를 수집하고 감독 학습문제에서 올바른 라벨링을 의미합니다.

• 데이터 처리 : 데이터 정리 (예 : 중복되거나 고도로 연관된 기능 제거 또는 누락 된 데이터 채우기)와 교육 데이터를 정의하는 기능에 대한 이해를 의미합니다.

• 테스트 케이스 생성 : 일반적으로 데이터는 2 개 또는 3 개의 세트로 나눌 수 있습니다 : `알고리즘을 교육하는 훈련 데이터` 세트, 알고리즘을 훈련 한 후 접근 방식의 `정확성을 테스트하는 테스트 데이터` 세트

또한 종종 검증 데이터셋을 만들어 교육 테스트 절차를 여러 번 반복하고 결과에 만족한 후 최종 테스트(또는 검증)를 수행하는 경우가 있습니다.

테스트 및 검증 데이터셋을 생성해야 하는 타당한 이유가 있습니다. 앞서 언급했듯이 기계 학습 기법은 원하는 결과의 근사치만 생성할 수 있습니다. 

이것은 종종 유한하고 제한된 수의 변수 만 포함 할 수 있으며 우리 자신의 통제 밖에있는 많은 변수가있을 수 있기 때문입니다.

단일 데이터 세트 만 사용한 경우 우리 모델은 데이터를 "암기"하게되고 암기 한 데이터에 매우 높은 정확도의 값을 생성 할 수 있지만이 결과는 다른 유사한 데이터 세트에서 재현되지 않을 수 있습니다.

기계 학습 기술의 핵심 목표 중 하나는 일반화 능력입니다.

이것이 교육 후에 모델 선택을 튜닝하는 데 사용되는 테스트 데이터 세트와 선택한 알고리즘의 유효성을 확인하기 위해 프로세스 마지막에만 사용되는 최종 유효성 검사 데이터 세트를 모두 만드는 이유입니다.

데이터에서 유효한 피쳐를 선택하는 중요성과 데이터를 "외우는 것"을 피하는 중요성을 이해하기 위해 (보다 기술적 인 측면에서 이것은 문헌에서 "오버 피팅 (overfitting)"이라고 불리는 것이며, 이것이 우리가 호출 할 것입니다 지금부터) xkcd 만화에서 가져온 농담을 예로 들어 보겠습니다.

"1996년 이전에는 현직이고 전투 경험이 없는 민주적인 미국 대통령 후보가 스크래블에서 더 가치가 있는 이름을 가진 사람을 이긴 적이 없었습니다." 

이 예에서 그러한 "규칙"은 의미가 없지만 유효한 특징을 선택하는 중요성을 강조합니다 (Scrabble에서 이름이 얼마나 가치가 있으며 미국 대통령 선출과 관련이 있습니까?). 예측자들은 현재의 데이터를 예측할 수는 있지만 보다 일반적인 데이터의 예측 자로 사용될 수는 없으며 52 개의 선거에 대해 이것이 실현되었다는 사실은 단순한 우연이었다.

이것은 일반적으로 오버 피팅 (overfitting)이라고 불리는 것입니다. 즉, 손에있는 데이터를 완벽하게 맞추는 예측을 수행하지만 더 큰 데이터 세트로 일반화하지는 않습니다. 오버 피팅은 일반적으로 "소음"(즉, 실제 의미가없는 정보)을 인식하고 모델을 작은 동요,흔들림에 맞추려 고 노력하는 과정입니다.

또 다른 예는 기계 학습을 사용하여 지상에서 다시 땅에 닿을 때까지 (수직이 아닌) 지상에서 던져진 공의 궤도를 예측하려고 시도 할 수 있습니다.

물리학은 궤도가 포물선 모양이라는 것을 우리에게 가르쳐 주며, 수천 개의 그런 던짐을 관찰하는 훌륭한 기계 학습 알고리즘이 해결책으로 포물선을 생각해 낼 것으로 기대합니다.

하지만, 만약 우리가 공을 확대해서 난기류로 인한 공기 중에서 가장 작은 변동을 관찰한다면, 우리는 공이 일정한 궤적을 가지고 있지 않지만 작은 동요를 일으킬 수 있다는 것을 알 수 있다. 이것이 우리가 "소음"이라고 부르는 것이다. 

이러한 작은 혼란을 모델링하려고 시도하는 기계 학습 알고리즘은 큰 그림을 보지 못하고 만족스럽지 못한 결과를 낳습니다. 

다시 말해, overfitting은 기계학습 알고리즘이 나무를 보지만 숲은 잊게 만드는 과정이다.

[오버피팅](C:\Users\dlrmacjf\Desktop\python>overfitting.PNG)


테스트 데이터의 정확도가 교육 데이터에 대해 달성된 결과와 비슷하지 않다면 모델을 오버피딩했다는 것을 알 수 있습니다. 따라서 교육 데이터를 테스트 데이터와 구분합니다.

물론, 우리는 반대의 오류를 만들지 말아야합니다. 즉, undefit the model,
undefit=샘플 크기에 비해 매개 변수가 너무 적은 통계 모델을 사용하는것.

`샘플 크기에 비해 매개 변수가 너무 적은 모델을 쓰지 말라 이거다.`

실제로 우리의 훈련 데이터에서 가능한 한 정확하게 예측 모델을 만드는 것을 목표로한다면 underfitting은 overfitting보다 훨씬 위험이 적으므로 모델을 초과 적용하지 않도록 대부분의 주의가 필요합니다.

+오버피팅보단 언더피팅이 낫다.

[언더피팅](C:\Users\dlrmacjf\Desktop\python>underfitting.PNG)

---

#h2 인기있는 기술 / 알고리즘에 대한 간략한 설명

"학습 스타일", 즉 책 초반에 논의 된 세 가지 수업, 감독 학습, 감독되지 않은 학습 및 강화 학습을 기반으로 알고리즘을 그룹화하는 것 외에도 구현별로 그룹화 할 수도 있습니다.

위에서 논의한 각 클래스는 서로 다른 기계 학습 알고리즘을 사용하여 구현될 수 있습니다. 예를 들어, 각기 다른 감독 학습 기법을 사용할 수 있습니다. 각 학습 기법이 당면한 특정 분류 또는 회귀 작업에 가장 적합할 수 있습니다.

실제로, 분류와 회귀를 구별하는 것이 가장 중요하며, 우리가 달성하고자하는 것을 이해하는 것이 중요합니다.

다음은 각 기계 학습 방법에 대한 철저한 설명이나 철저한 설명을 의미하는 것은 아니며, 독자는이를 Python Machine Learning, Sebastian Raschka (https://www.packtpub.com/ big- 데이터 및 비즈니스 인텔리전스 / 파이썬 기계 학습), 오히려 독자들에게 다양한 기술의 단순한 맛을 제공하고 깊이있는 학습이 다른 기술과 얼마나 다른지에 대한 간단한 검토를 의미합니다.

다음 장에서는 심층 학습이 단순히 알고리즘을 학습하는 것이 아니라 고전적인 기계 학습 기술과 크게 다르다는 것을 알게 될 것입니다.

회귀 알고리즘, 선형 회귀, 고전 분류기같은 의사결정 트리, 나이브 베이즈 , 지원벡터 머신, 자율 클러스터링 알고리즘 같은 k-mean, 강화 학습 기술, 크로스 엔트로피 방법 등을 도입하여 존재하는 기계 학습 기술의 다양성을 조금 엿볼 수 있으며, 신경망을 도입하여 이 목록을 끝낼 것입니다. 이것이 책의 주안점입니다.

--

#h2 선형회귀(Linear regression )

회귀 알고리즘은 크기, 나이, 욕실 수, 층 수, 위치 등과 같은 특정 기능이있는 집의 비용과 같이 가치를 예측하기 위해 입력 데이터의 기능을 사용하는 감독 알고리즘의 일종입니다.

회귀 분석은 입력 데이터 집합에 가장 잘 맞는 함수의 매개 변수 값을 찾으려고 시도합니다.선형 회귀 알고리즘에서 목표는 목표 값에 가장 근접한 입력 데이터에서 함수에 대한 적절한 매개 변수를 찾아서 비용 함수를 최소화하는 것입니다.

비용 함수는 정확한 결과를 얻지 못하는 정도의 오류 함수입니다. 사용되는 일반적인 비용 함수는 평균 제곱 오차이며, 여기서 예상 값과 예측 결과의 차이를 제곱한다. 모든 입력 예제에 대한 합계는 알고리즘 오류를 제공하고 비용 함수를 나타냅니다.

25 년 전에 건축 된 100 평방 미터의 집이 3 개의 욕실과 2 개 층으로 구성되어 있다고 가정 해보십시오.

  또한 집이 10 개의 다른 지역에있는 도시를 1에서 10까지의 정수로 나눈 다음이 집이 7로 표시된 지역에 있다고 가정합니다. 그러면이 집을 다음과 같이 매개 변수화 할 수 있습니다. 5 차원 벡터 x = (100, 25, 3, 2, 7). 이 집이 € 10,0000의 예상 가치를 가지고 있음을 또한 알았다고 가정 해보십시오. 우리가 원하는 것은 f (x) = 100000과 같은 함수 f를 생성하는 것입니다.

선형 회귀 분석에서 이것은 100 * w1 + 25 * w2 + 3 * w3 + 2 * w4 + 7 * w5 = 100000이되도록 벡터 w = (w1, w2, w3, w4, w5)

 우리가 천개의 집을 가지고 있다면, 모든 집에 동일한 과정을 반복 할 수 있고 이상적으로는 모든 집에 대해 정확한 값을 예측할 수있는 벡터를 찾고 싶습니다. 우리가 초기에 임의의 w 값을 선택했다고합시다.

이 경우 f (x) = 100 * w1 + 25 * w2 + 3 * w3 + 2 * w4 + 7 * w5가 1,00,000과 같지 않을 것으로 예상 할 수 있습니다.

->w가 정확할리 없기 때문이지.

따라서 Δ = (100000 - f (x)) 2의 오차를 계산할 수 있습니다. 이것은 하나의 예제 x에 대한 제곱 오차이고, 모든 예제에 대한 모든 제곱 오차의 평균은 우리의 비용, 즉 우리의 함수가 실제 값과 얼마나 다른지를 나타냅니다. 

->왜 /2지?  

그러므로 우리의 목표는이 오차를 최소화하는 것이며, 이렇게하기 위해 우리는 w에 대한 비용 함수의 미분 δ를 계산한다

미분은 함수가 증가하거나 감소하는 방향을 나타냅니다.

therefore, moving w in the opposite direction to the derivative will improve our function's accuracy. 

따라서 미분을 반대방향으로 이동한다면 함수의 정확도가 향상된다.

이것은 선형 회귀의 주요 포인트로, 오류를 나타내는 비용 함수의 최소 값으로 이동합니다.

물론 미분은 방향을 나타낼 뿐이기 때문에 미분 방향으로 얼마나 빨리 움직이고 싶은지는 결정해야 한다.

비용함수는 선형이 아니므로, 따라서 우리는 미분이 지시하는 방향으로 작은 단계 만을 취할수 있도록 해야한다.

너무 큰 발걸음을 내딛는 것은 우리가 최소한을 초과하게 만들 수 있고, 따라서 그것에 집중하지 못할 수도 있다.

이 단계의 크기는 학습 속도라고 불리는 것이며, 우리는 기호 "lr"로 크기를 나타낼 수 있습니다.

따라서 w = w - δ * lr을 설정함으로써보다 나은 솔루션을 향한 w의 선택을 향상시킵니다.

-왜 곱하는지 이해는 불가. 오차를 줄여 가장 가까운 w값을 만들려는건 알겠는데..

이 과정을 여러 번 반복하면 함수 f에 대한 최선의 선택을 나타내는 w 값이 생성됩니다.

여기서 강조할 점은, 이 프로세스 로컬에서만 작동하며 공간이 볼록하지 않은 경우 전역적인 최상의 가치를 찾지 못할 수도 있음

이미지가 암시 하듯이, 많은 로컬 미니 마가 존재한다면, 알고리즘은 이러한 로컬 미니 마 중 하나에 갇히게 될 수 있으며,에러 함수의 글로벌 최소값에 도달하기 위해 그것을 벗어날 수 없습니다.

 마치 작은 공이 산에서 내려 오다가 작은 계곡에 갇히고 결코 산의 바닥에 닿을 수는 없는 것처럼.




[그래프](C:\Users\dlrmacjf\Desktop\python>minima.PNG)

상단 그래프는 볼록형이므로 하나의 최소값 만 존재합니다. 아래 그래프에서 함수는 두 개의 로컬 미니 마를 가지므로 초기화에 따라 프로세스가 전역 최소가 아닌 첫 번째 로컬 최소값을 찾을 수 있습니다.

--
#h2 결정트리(Decision trees )

널리 사용되는 또 다른 감독 알고리즘은 의사 결정 트리 알고리즘입니다.

의사 결정 트리 알고리즘은 "트리"형태로 분류자를 생성합니다.

의사 결정 트리는 특정 속성에 대한 테스트가 수행되는 결정 노드와 대상 속성의 값을 나타내는 리프 노드로 구성됩니다.

의사결정 트리는 루트 노드(맨위)에서 시작하여 리프 노드(맨아래)에 도달할 때까지 의사결정 노드를 통해 아래로 이동하는 분류자의 한 유형이다

이 알고리즘의 대표적인 적용 분야는 아이리스 꽃 데이터 세트(http://archive. ics.uci.edu/ml/datasets/Iris)이며, 세 가지 유형의 아이리스(Iris setosa, Iris Virinica, Iris vericor) 중 50개 표본의 데이터가 포함되어 있습니다.

데이터 세트를 만든 로널드 피셔 (Ronald Fisher)는이 꽃들의 네 가지 다른 특징,꽃의 길이와 너비, 꽃잎의 길이와 너비를 측정했습니다.

여기서 우리는 정확하게 분류 할 수있는 단순화 된 의사 결정 트리를 설명 할 것입니다. 거의 모든 꽃은 꽃잎의 길이와 너비 중 두 가지를 사용합니다.

첫 번째 노드부터 시작하여 꽃잎 길이에 대한 첫 번째 테스트를 만듭니다.

만약 꽃잎 길이가 2.5보다 작다면, 꽃은 아이리스 세토사 종에 속합니다

사실 이것은 모두 꽃잎 길이가 2.5cm 미만인 모든 세토사 꽃들을 정확하게 분류합니다. 그러므로, 우리는 결과 Iris setosa에 의해 분류 된 잎 노드에 도달한다.

꽃잎의 길이가 2.5보다 큰 경우 다른 분기를 가져 와서 새 결정 노드에 도달하고 꽃잎의 너비가 1.8보다 큰지 여부를 테스트합니다.

꽃잎의 너비가 1.8보다 크거나 같으면 우리는 잎 노드에 도달하고 우리는 꽃을 아이리스 버지니아라고 분류합니다. 그렇지 않으면 꽃잎의 길이가 4.9보다 긴지 여부를 다시 테스트하는 새로운 결정 노드에 도달합니다.

그렇다면 Iris virginica 꽃으로 표시된 잎 노드에 도달합니다. 그렇지 않으면 Iris versicolor 꽃으로 표시되는 다른 잎 노드에 도달합니다.

논의 된 결정 트리는 왼쪽 노드가 결정 노드에서의 테스트에 대한 긍정적 인 대답을 반영하는 반면, 오른쪽 분기는 결정 노드에서의 테스트에 대한 부정적인 응답을 나타냅니다.각 분기의 끝 노드는 리프 노드입니다.

[결정트리](C:\Users\dlrmacjf\Desktop\python>division tree.PNG)

이 예는 의사 결정 트리 알고리즘이 선형 회귀와 얼마나 다른지를 보여줍니다. 또한, 신경망을 도입 할 때 독자는이 동일한 데이터 집합을 사용하여 신경망의 작동 방식을 볼 수 있습니다. 이 예제에서 우리는 파이썬 코드도 제공 할 것이고 신경망이 그들의 특징에 따라 꽃을 어떻게 분리하려고하는지 몇 가지 그림을 보여줄 것입니다.

---

#h2 K-means

이미 설명한 클러스터링 알고리즘은 감독되지 않은 기계 학습 방법의 한 유형입니다.

가장 일반적인 클러스터링 기법은 k-means 클러스터링 (k-means clustering)이라고 불리는 클러스터링 기법으로 데이터 집합의 모든 요소를 k 개의 고유 한 하위 집합으로 그룹화하여 클러스터링하는 기술입니다 (따라서 이름의 k).

K-means는 상대적으로 간단한 과정이며, k 개의 부분 집합의 중심을 나타내는 무작위 k 점을 선택하는 것으로 구성되며,이 무작위점을 sentriod(중심)이라고합니다.

그런 다음 각 중심에 대해 가장 가까운 모든 점을 선택합니다. 그러면 k 개의 다른 하위 집합이 만들어집니다. 이 시점에서 각 하위 집합에 대해 센터를 다시 계산합니다.

우리는 다시 새로운 중심을 가지고 있습니다. 그리고 우리는 위의 단계를 반복하여 각각의 중심에 가장 가까운 지점의 새로운 부분인 중심을 선택합니다. 우리는 중심이 움직이지 않을 때까지 이 과정을 계속합니다.

->여기서 중심부란 sentriod임.

이 기법을 제대로 사용하려면 점 사이의 거리를 계산할 수있는 척도를 식별 할 수 있어야합니다. 이 절차는 다음과 같이 요약 할 수 있습니다.

1. centroids라고 불리는 초기 k- 점을 선택하십시오.
2. 데이터 세트의 각 점에 가장 가까운 중심을 연결합니다.
3. 특정 중심에 연결된 점 세트에 대한 새 중심을 계산합니다.
4. 새로운 중심을 새로운 중심으로 정의하십시오.
5. 중심이 움직이지 않을 때까지 3 단계와 4 단계를 반복하십시오.

=

이 방법은 임의의 centriods(중앙점)의 초기 선택에 민감하며, 다른 초기 선택(다른 임의의 중앙점을 다시 선택한다.)에 대해 과정을 반복하는 것이 좋은 생각일 수 있다는 점에 유의해야 한다.

또한 일부 중심점이 데이터 집합의 어떤 점에도 가장 가까이 있지 않는다면, 하위 집합의 수를 k 개에서 줄이는 것이 가능합니다.

만약 우리가 위 예에서 k=3과 함께 k-means를 사용한다면, 그것은 또한 언급할 가치가 있다. 

의사결정 트리를 사용하여 발견한 홍채 데이터 세트에 대해 동일한 분류를 얻지 못할 수 있습니다. 이것이 각 문제에 대해 올바른 기계 학습 방법을 선택하고 사용하는 것이 얼마나 중요한지를 한 번 더 강조합니다.

이제 k-means 클러스터링을 사용하는 실용적인 예제를 살펴 보겠습니다.

피자 배달 장소가 신도시에 4 개의 프랜차이즈를 개설하기를 원하며 4 개의 새 사이트의 위치를 선택해야한다고 가정 해 봅시다.

이것은 k-means 클러스터링을 사용하여 쉽게 해결할 수있는 문제입니다.

아이디어는 피자가 가장 자주 주문되는 위치를 찾는 것입니다. 이들은 우리의 데이터 포인트가 될 것입니다. 다음으로 사이트 위치가있는 4 개의 임의 지점을 선택합니다.

k-means 클러스터링 기법을 사용하여 나중에 각 배달 장소까지의 거리를 최소화하는 4 개의 최적 위치를 식별 할 수 있습니다. 이것은 k-means 클러스터링이 비즈니스 문제를 해결하는 데 도움이 될 수있는 예입니다.

[ㅏals](C:\Users\dlrmacjf\Desktop\python>k-means)

#h2 나이브베이즈(Naïve Bayes )

Naive Bayes는 다른 많은 기계 학습 알고리즘과 다릅니다.

확률 론적으로, 대부분의 기계 학습 기술이 평가하려고 시도하는 것은 어떤 이벤트 Y가 주어진 조건 X의 확률이다. 우리는 p (Y | X)로 표시한다.

  예를 들어 숫자를 나타내는 그림 (즉, 특정 픽셀 분포를 갖는 그림)이 주어진다면 해당 숫자가 5 일 확률은 얼마입니까?

픽셀의 분포가 5로 표시된 다른 예제의 픽셀 분포에 가깝게되는 경우 해당 이벤트의 확률이 높습니다. 그렇지 않으면 확률이 낮습니다.

때때로 우리는 반대의 정보를 가지고 있습니다. 즉, 우리는 사건 Y를 가지고 있다는 것을 알고, 우리의 표본이 X라는 확률을 압니다. 나이브베이즈 정리는 다음과 같이 말합니다 :

p (X | Y) = p (Y | X) * p (X) / p (Y)

여기서 p (X | Y)는 Y가 주어진 인스턴스 X를 생성 할 확률을 의미하며, 이 때문에 naive bayes를 생성 적 접근이라고합니다.

In simple terms, we may calculate the probability that a certain pixel configuration represents the number 5, knowing what is the probability, given that we have a 5, that a random pixel configuration may match the given one. 

간단히 말하자면, 임의의 픽셀 구성이 5를 나타내는 확률을 계산할 수 있으며, 주어진 확률이 5임을 알면 임의의 픽셀 구성이 주어진 픽셀 구성과 일치 할 수 있습니다.

이것은 의료 검사의 영역에서 가장 잘 이해됩니다.

우리가 특정한 질병이나 암을 검사한다고 가정해보자. 우리는 우리의 테스트 결과가 긍정적이라는 것을 고려할 때 우리가 특정한 질병에 걸릴 확률을 알고 싶다.

현재, 대부분의 테스트는 신뢰성 값을 가지고 있는데, 이것은 특정 질병에 걸린 사람들을 대상으로 시행될 때 양성 반응을 보일 확률의 백분율입니다.

p (X | Y) = p (Y | X) * p (X) / p (Y)를 반대로하면,우리는 그것을 가지고 있습니다 :

p(cancer | test=positive) = p(test=positive | cancer) * p(cancer)/p(test=positive) 

검사는 98 % 신뢰도가 있다고 가정합니다. 즉, 환자의 98 %가 암을 앓고 있으면 검사는 양성이며, 마찬가지로 암이없는 사람도 검사 결과가 부정적입니다.

또한이 특별한 종류의 암은 노인들에게만 영향을 미친다고 가정합니다.

50 세 미만의 사람들 중 2 %만이 이런 종류의 암을 앓고 있으며 50 세 이하의 사람들에게 시행되는 검사는 인구의 3.9 %에서만 긍정적입니다 (우리는이 사실을 데이터에서 파생시킬 수 있었지만 간단하게하기 위해 정보를 제공했습니다)

우리는 이 질문을 할 수 있습니다 : 검사가 암에 대해 98 % 정확하고 45 세의 사람이 검사를 받았고 그것이 양성으로 밝혀지면 암에 걸릴 확률은 얼마입니까? 위 공식을 사용하여 계산할 수 있습니다.

p(cancer | test=positive) = 0.98 * 0.02/0.039 = 0.50

따라서, 테스트의 높은 정확성에도 불구하고, Nayve Bayes는 50세 미만에서는 암이 매우 드물다는 사실을 고려할 필요가 있다고 말합니다. 따라서 테스트의 양성이 98%의 확률을 제공하지 못합니다.

일반적으로 추정하려는 결과에 대한 확률 p(cancer) 또는 그 이상의 확률을 사전 확률이라고 합니다. 추가 정보가 없는 사건의 확률을 나타내기 때문입니다.그러므로 우리가 테스트를 보기전에

이 시점에서 우리는 더 많은 정보가 있다면 어떻게 될지 궁금해 할 것입니다.

예를 들어 우리가 다른 신뢰도로 다른 검사를 수행하거나 가족의 암 재발과 같은 사람에 대한 정보를 알고 있다면.

위의 방정식에서 우리는 계산의 요인 중 하나 인 확률 p (test = positive | cancer)를 사용했습니다.

그리고 두 번째 검사를 실시하고 그것이 양성으로 확인되면, 우리는 또한 p (test2 = positive | cancer)을 가질 것입니다.

naive bayes는 각 정보가 서로 독립적이라는 가정을 만든다.(즉, 테스트 2는 테스트 1의 결과를 알지못했고, 독립적이라는 것이다. 이 뜻은, 테스트1은 테스트2의 결과를 바꿀수 없었고 그렇기에 첫번째 테스트의 결과는 편향되지 않았다.)

  naive Bayes는 서로 다른 사건의 독립성을 추정하여 그 확률을 계산하는 분류 알고리즘입니다. 그래서:

p(test1 and test2=pos | cancer) =p(test1=pos | cancer)*p(test2=pos | cancer)

이 방정식은  가능성 L?(likelihood L)이라고도하며, (test1 및 test2 = pos) 사람이 암에 걸렸다는 사실을 감안할 때 test1 및 test2가 양성인 것으로 나타납니다.

p(cancer | both tests=pos) = = p(both test=pos | cancer)*p(cancer)/p(both tests=pos)  = = p(test1=pos | cancer)*p(test2=pos | cancer) *p(cancer)/p(both tests=pos) 


아! 존나모르겠다
--

#h2 보조벡터머신(Support vector machines)

지원 벡터 기계는 주로 분류에 사용되는 지도 기계 학습 알고리즘입니다.

다른 머신 학습 알고리즘에 비해 지원 벡터 머신의 장점은 데이터를 클래스로 분리 할뿐만 아니라 최대화하는 분리 하이퍼 평면 (3 차원 이상의 공간에서 평면과 유사)(the analog of a plane in a space with more than three dimensions) 을 찾는 것입니다.

각 점을 하이퍼 평면으로부터 분리하는 margin(여백), 또한 지원 벡터 머신은 데이터가 선형으로 분리되지 않는 경우에도 처리 할 수 있습니다.

비선형으로 분리 가능한 데이터를 처리하는 두 가지 방법이 있습니다. 하나는 부드러운 여백을 도입하는 것이고 다른 하나는 소위 커널 트릭을 도입하는 것입니다.

부드러운 여백은 알고리즘의 예측 가능성을 유지하면서 몇 가지 분류되지 않은 요소를 허용함으로써 작동합니다.

소프트 마진은 알고리즘의 예측 가능성을 유지하면서 몇 가지 분류되지 않은 요소를 허용함으로써 작동합니다.

위에 설명했듯이, 실제로는 기계 학습 모델을 과도하게 조율하지 않는 것이 더 좋으며, 지원 벡터 머신 가설 중 일부를 완화하여 수행 할 수 있습니다.

커널 트릭은 대신 피처 공간을 다시 하이퍼 평면이 아닌 하이퍼 평면을 정의 할 수있는 또 다른 공간으로 피처 공간을 매핑하는 작업을 포함하며, 분리 가능한 것으로 보이지 않는 데이터 세트의 요소를 분리 할 수 ​​있습니다.

너무 많은 시간이 걸리 겠지만 벡터 시스템을 지원한다는 개념이 비선형 상황으로 일반화 할 수있는 능력으로 인해 매우 인기 있고 효과적이라는 개념을 강조하고 싶습니다.

우리가 전에 보았 듯이, 감독 된 기계 학습 알고리즘의 임무는 피처 공간에서 일련의 클래스에 이르는 함수를 찾는 것입니다.

각 입력 x = (x1, x2, ..., xn)은 입력 예제를 나타내고 각 \\({x}_i\\)는 \\({i}^th\\) 기능의 x 값을 나타냅니다.

 \\({i}^th\\) 기능이 욕실 수와 일치하면  \\({x}_i\\)는 집 x에있는 욕실 수에 해당합니다.

 이전에 우리는 욕실이나 위치의 수와 같은 일부 기능에 따라 특정 주택의 재판매 가치를 추정하려고 시도했습니다.

우리는 함수 k를 만들수있고 기능 K는 기능 공간에서 이 공간의 다른 표현까지 생성할 수 있습니다. 커널이라고 불리는.

예를 들어, k는 \\({x}_i\\)를 \\({x}_i\\) 2로 맵핑하고, 일반적으로 기능 공간을 다른 공간 W에 비선형 적으로 맵핑 할 수있다.

따라서 분리된 w의 하이퍼 평면은 더 이상 선형 하이퍼 평면이 아닌 기능 공간으로 다시 매핑 될 수 있습니다.'

이것이 사실 인 정확한 조건은 잘 정의되어 있지만이 짧은 소개의 범위를 벗어납니다.

그러나 이는 다시 고전 기계 학습 알고리즘에서 올바른 기능을 선택하는 중요성을 다시 한번 강조하며, 특정 문제에 대한 해결책을 찾는 데 도움이 될 수 있습니다.

[서포트벡터](C:\Users\dlrmacjf\Desktop\python>support vector.PNG)

왼쪽에는 커널이 적용되기 전에 비선형으로 분리 가능한 세트가 있습니다. 오른쪽은 커널이 적용된 후 동일한 데이터 세트이고 데이터는 선형으로 분리 될 수 있습니다


--


#h2 교차 엔트로피 기법(cross-entropy method)

지금까지 우리는 지도및 non-지도 학습 알고리즘을 소개했다.

교차 엔트로피 방법은 그대신 알고리즘의 강화 학습 클래스에 속하며, 이에 대해서는 7 장, 8장에선 보드 게임에 대한 딥러닝, 책의 컴퓨터 게임에 대한 딥러닝을 자세히 설명합니다.

십자형 방법은 최적화 문제를 해결하는 기술, 즉 특정 기능을 최소화하거나 최대화하기위한 최상의 매개 변수를 찾는 기술입니다.

1. 딥 러닝을 위해 최적화하려는 변수의 랜덤 표본을 생성합니다. 이러한 변수는 신경 네트워크의 가중치일 수 있다.
2. 태스크를 실행하고 성능을 저장하십시오.
3. 최적의 실행을 확인하고 가장 실적이 좋은 변수를 선택하십시오.
4. 실적이 가장 우수한 실행을 기반으로 각 변수의 새로운 평균과 분산을 계산하고 변수의 새 샘플을 생성합니다.
5. 정지 조건에 도달하거나 시스템이 개선되지 않을 때까지 단계를 반복하십시오

많은 변수에 따라 달라지는 함수에 대해 해결하려고 한다고 가정하자.

예를 들어, 우리는 특정 고도에서 발사되었을 때 가장 오래 날 수 있는 모형 비행기를 만들기 위해 노력하고 있다.

비행기가 커버하는 거리는 날개 크기, 각도, 무게 등의 기능이 될 것이다. 매번, 우리는 각 변수를 기록하고 비행기를 띄우고 그것이 날아가는 거리를 측정할 수 있다.

그러나 가능한 모든 조합을 시도하기보다는 통계를 작성하고 최우선 및 최악의 실행을 선택하며 최적의 실행 과 최악의 실행 중에 어떤 값으로 변수가 설정되었는지 확인합니다.

예를 들어, 각 최고 주행에 대해 비행기의 날개가 특정 크기인 것을 탐지하는 경우, 특정 날개 크기가 비행기의 장거리 비행에 최적일 수 있다고 결론 내릴 수 있다.

반대로, 각각의 최악의 달리기의 경우, 비행기의 날개가 특정 각을 이루고 있다면, 우리는 그 특별한 각이 우리 비행기의 날개에 나쁜 선택이 될 것이라고 결론을 내릴 것입니다.

일반적으로 우리는 최적의 평면을 생성해야하는 각 값에 대한 확률 분포를 생성 할 것이며, 가능성은 더 이상 무작위가 아니라 우리가 받은 피드백을 기반으로합니다.

따라서이 방법은 일반적인 강화 학습 프로세스에서 문제 (각 변수의 값)에 대한 최적의 솔루션을 결정하기 위해 런으로부터의 피드백 (비행기가 얼마나 멀리 비행했는지)을 사용합니다.

--

#h2 신경망 네트워크(neural network)

몇 가지 유명한 고전 기계 학습 알고리즘으로 독자들을 새롭게 만든 후에, 우리는 이제 신경 네트워크를 소개하고 그것들이 어떻게 작동하는지 그리고 우리가 간단하게 요약한 알고리즘과 어떻게 다른지 더 자세히 설명할 것이다.

신경망은 알고리즘을 학습하는 또 다른 기계로 인기가 높고 사용 빈도가 적은시기를 알고 있습니다.

 우리가 다음 장과 그 다음 장에 바칠 신경망을 이해하는 것은 이 책의 내용을 따르기 위한 핵심이다.

신경 회로망의 첫 번째 예는 1957 년 Frank Rosenblatt가 발명 한 퍼셉트론 (perceptron)이라고 불렀습니다. 퍼셉트론은 입력 및 출력 레이어로만 구성된 네트워크입니다.

바이너리 분류의 경우 출력 레이어에는 하나의 뉴런이나 단위 만 있습니다. 퍼셉트론은 처음부터 매우 유망한 것으로 보였지만 직선적으로 분리 가능한 패턴만을 배울 수 있다는 것을 빨리 깨달았습니다.

예를 들어, Marvin Minsky와 Seymour Papert는 XOR 논리 함수를 배울 수 없다는 것을 보여주었습니다. 가장 기본적인 표현에서, 퍼셉트론은 단순히 하나의 뉴런과 입력, 즉 여러 뉴런으로 구성 될 수있는 입력의 표현입니다.

뉴런에 다른 입력이 주어지면 수식으로 활성화 값을 정의합니다. \\(a(x) = \sum_{i} w_i x_i\\) 여기서 x_i는 입력 뉴런의 값이고, w_i는 뉴런 i와 출력 간의 연결 값입니다.

우리는 다음 장에서 훨씬 더 자세하게 이것을 배울 것입니다. 왜냐하면 이제는 퍼셉트론이 로지스틱 회귀 알고리즘과 많은 유사점을 공유한다는 것을 알아야합니다.

그리고 퍼셉트론은 선형 분류자에 의해 제한된다. 뉴런 내부 상태로 간주되어야 할 활성화 값이 고정 임계값 b보다 크면 뉴런은 활성화될 것입니다. 즉, 작동하거나 그렇지 않으면 작동하지 않습니다.


[퍼셉트론][C:\Users\dlrmacjf\Desktop\python>perseptron.PNG]

세 개의 입력 장치 (뉴런)와 하나의 출력 장치 (뉴런)가있는 간단한 퍼셉트론

따라서 <w, x> = 0 인 모든 벡터 x는 R3에서 하이퍼 평면을 정의합니다 (3은 x의 차원이지만 일반적으로는 임의의 정수가 될 수 있습니다).

따라서, <w, x>> 0를 만족하는 임의의 벡터 x는 w에 의해 정의 된 하이퍼 - 평면의 측면상의 벡터이다. 이것은 퍼셉트론이 어떻게 하이퍼평면을 정의하고 그것을 분류 자로 사용 하는지를 명확하게합니다.

일반적으로 0 대신에 임의의 실수 b로 임계 값을 설정할 수 있습니다.이 값은 하이퍼 평면을 원점에서 멀리 이동시키는 효과가 있습니다.

그러나이 값을 추적하기보다는 일반적으로 네트워크에 바이어스 단위를 포함합니다.이 단위는 연결된 가중치 -b가있는 항상 (값 = 1) 특수 뉴런입니다.


이 경우 연결 가중치가 -b 값을 가지면 활성화 값은 a(x) = \\(sum_i w_i x_i\\) 가되고 a (x)> 0으로 설정하는 것은  \\(sum_i w_i x_i\\) > b.로 설정하는 것과 같다.

[가중ㅊ](C:\Users\dlrmacjf\Desktop\python>input layer.PNG)
출력 벡터에 바이어스 단위가 추가 된 퍼셉트론 바이어스 단위는 항상 켜져 있습니다.

퍼셉트론은 퍼포먼스가 제한적이지만 신경망의 첫 번째 사례이기 때문에 역사적으로 매우 중요합니다.

신경망은 물론 하나의 출력 뉴런을 가질 필요가 없으며 실제로 일반적으로 출력뉴런은 없습니다.

네트워크에 하나 이상의 출력 뉴런이있는 경우 각 출력 뉴런에 대해 동일한 프로세스를 반복 할 수 있습니다.

각각의 가중치는 두 개의 지수, i와 j로 표시됩니다.가중치가 입력 레이어의 뉴론 i를 출력 레이어의 뉴론 j에 연결하고 있음을 나타냅니다.
출력 레이어의 각 뉴런에 값 1로 연결된 바이어스 장치와의 연결도 있습니다.
활성화 값에 대해 다른 활동 함수를 정의 할 수 있다는 점도 유의해야합니다.

우리는 활성화 값을 a (x) = \\(sum_i w_i x_i -b\\)로 정의했다. (이제부터는 바이어스가이 공식에 포함된다고 가정 할 것이다) 활성화가 0보다 큰 경우 뉴런이 활성화된다고 말했다.

앞으로 살펴 보 겠지만, 이것은 이미 활성화 함수 즉, 뉴런의 내부 상태에 대해 정의 된 함수를 정의합니다. 활성화를 0보다 크게하면 뉴런이 활성화되기 때문에 이것을 임계점 활동이라고합니다

그러나 신경망에는 활성화 값으로 정의 할 수있는 다양한 활동 함수가있을 수 있으며 다음 장에서 자세히 다룰 것입니다.